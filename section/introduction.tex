Large Language Models (LLMs) have become fundamental components of modern artificial intelligence systems, enabling fluent text generation across a wide range of applications. Despite their impressive linguistic and contextual capabilities, these models are known to produce \textbf{hallucinations} -- outputs that are syntactically valid but factually incorrect or unsupported. This phenomenon arises from the probabilistic nature of autoregressive text generation, where models predict the most likely next token rather than verifying factual accuracy. As a result, even well-trained systems may generate statements that contradict real-world knowledge.

Hallucination poses a significant problem for both \textbf{research} and \textbf{practical deployment}. In low-risk contexts such as creative writing, factual deviations may be acceptable, but in domains such as medicine, law, or education, misinformation can have severe consequences. Consequently, mitigating hallucination has become a central challenge for ensuring the reliability and trustworthiness of generative models. The issue affects nearly every aspect of LLM usage -- from summarization and question answering to conversational assistants -- where factual consistency is critical to user confidence.

A growing body of work has investigated the causes and mitigation strategies of hallucinations in neural text generation. \textit{Cao, Narayan, and Bansal} \cite{cao2021hallucination} identified that hallucination often stems from mismatches between model fluency and knowledge grounding. More recent surveys, such as \textit{Tonmoy et al.} \cite{islam2024comprehensive} and \textit{Cossio} \cite{cossio2025comprehensive}, have highlighted that both \textbf{training data quality} and \textbf{model architecture} contribute significantly to the prevalence of hallucination. Low-reliability data sources -- such as noisy web crawls and user-generated content -- introduce factual inconsistencies that models may reproduce during generation, while architectural limitations allow factual drift in later decoding stages.

Motivated by these findings, this study investigates how \textbf{data reliability} and \textbf{architectural design} jointly influence hallucination rates in LLMs. We first establish empirical baselines by training identical models on corpora of differing factual reliability, isolating the effect of data quality on factual consistency. Building upon these results, we introduce a novel architectural component, the \textbf{Layer-Specific Factual Gate (LSFG)}, designed to suppress activations that lead to factual errors in the model’s final decoder layers. The LSFG mechanism dynamically filters representations contributing to factual inconsistency, thereby constraining the model’s output toward verifiable content.

Through systematic experimentation, we demonstrate that combining high-quality data with the proposed architectural intervention yields a \textbf{96.4\% reduction} in hallucination rates across multiple domains. These findings confirm that hallucination can be substantially mitigated by jointly improving the \textbf{factual integrity of the training corpus} and the \textbf{internal structure of the model}, offering a practical pathway toward more trustworthy language generation.

\subsection{Related work}

Since the introduction of the Transformer architecture \cite{vaswani2017attention}, large language models have achieved remarkable fluency but remain prone to generating factually incorrect statements. Early work by \textit{Cao, Narayan, and Bansal} \cite{cao2021hallucination} identified hallucination as a fundamental limitation of neural text generation, showing that models can produce fluent but ungrounded content even when trained on reliable sources.

Later research expanded on these findings by categorizing the causes and mitigation strategies of hallucinations. \textit{Tonmoy et al.} \cite{islam2024comprehensive} provided a comprehensive survey, distinguishing between data-centric, architecture-centric, and decoding-level approaches. Their results emphasized that while high-quality training data reduces hallucination rates, structural changes to the model are also necessary to ensure factual consistency.

\textit{Cossio} \cite{cossio2025comprehensive} introduced a taxonomy separating intrinsic hallucinations -- arising from internal model biases -- from extrinsic ones caused by unreliable data. This distinction clarified that both data integrity and model design jointly influence factual reliability.

Our work builds on these foundations by systematically examining the relationship between data quality and hallucination prevalence, and by introducing an architectural solution, the \textbf{Layer-Specific Factual Gate (LSFG)}, that constrains factual drift within the decoder. This integrated perspective advances prior research toward a more comprehensive approach to hallucination mitigation.