\subsection{Interpretation of Results}

The experimental findings demonstrate a clear and consistent reduction in hallucination rates across all tested domains, confirming the effectiveness of both data quality improvement and architectural refinement strategies. The comparative analysis between the baseline and modified models suggests that these two factors contribute to hallucination mitigation in distinct yet complementary ways. Specifically, it was observed that different domains benefited at varying rates from the respective interventions. Some showed greater sensitivity to improved data reliability, while others responded more strongly to architectural optimization. This indicates that neither aspect alone is sufficient and the suppression of hallucinations in large language models requires the joint advancement of data integrity and model design. The results thus reinforce the conclusion that a holistic approach, integrating both data-centric and architecture-centric perspectives, is essential to achieving robust factual consistency in generative models.

\subsection{Limitations and Further Research}

While the presented results are promising, several limitations must be acknowledged. First, benchmarking hallucinations remains an emerging area of research, and the tools available—such as HaluEval 2.0—still face inherent challenges in exhaustively detecting and quantifying hallucination phenomena. The reliance on automated factuality checks introduces potential measurement uncertainty, particularly for nuanced or context-dependent errors that remain difficult to classify. Second, although our training data was curated for quality and factual accuracy, constructing a truly knowledgeable language model often requires access to information that exists only in low-reliability or semi-structured corpora. Balancing the inclusion of such information without introducing factual instability remains a key open challenge. 

Future investigations should therefore focus on developing more comprehensive hallucination benchmarks that incorporate both factual and contextual dimensions. Expanding the experimental framework to larger model scales, diverse domains, and hybrid retrieval-augmented architectures could further clarify the interplay between data quality, model structure, and hallucination suppression. A deeper understanding of these interactions will be essential for the development of trustworthy, knowledge-grounded language models.