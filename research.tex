\documentclass[10pt,leqno,twoside]{article}

\usepackage{annal-latex}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{draftwatermark}
\usepackage{color}
\usepackage{multirow}
\usepackage[backend=bibtex, sortcites=true, style=numeric-comp, sorting=none]{biblatex}
\usepackage{authblk}



\addbibresource{research.bib}

\SetWatermarkText{\textbf{FAKE RESEARCH}}
\SetWatermarkLightness{0.85}
\SetWatermarkAngle{60}
\SetWatermarkScale{3.0}

\begin{document}
\setcounter{page}{1}

\vspace{-4cm} \fofej{49}{19}{nn}{nnn}

\vspace{.4cm}


\title{Achieving Near-Zero Hallucation In Large Language Models}

\author{{\bf Name} (City, Country)\\[1ex]
{\bf Name} (City, Country)
{\bf Name} (City, Country)
{\bf Name} (City, Country)}

\date{}



\abstract{
This paper presents a research methodology focused on the mitigation of hallucinations in modern large language models.  The initial phase involved the development and training of a models following the framework established in Google’s “Attention Is All You Need” \cite{vaswani2017attention} paper. The degree of hallucination present in the model outputs was then systematically measured with respect to the training data. These measurements were obtained after multiple training iterations conducted on models of identical size but trained on datasets differing in quality and factual reliability, thereby yielding models with varying levels of knowledge representation. The results indicated that both data quality and model architecture contributed significantly to the prevalence of hallucinations. Consequently, architectural modifications were introduced, after which a model was trained on high-quality data. This revised configuration was evaluated using the HaluEval 2.0 \cite{li2024dawn} benchmark which gave us an average macro hallucation rate of 0.45, and an average micro hallucation rate of 0.11 across the tested domains.
\textcolor{red}{\newline This research paper is for a Research Methodology course at ELTE University. The data in it is made up, and should not be taken seriously or referenced.}}


% ---------------------------------------------------------------


\section{Introduction}
\input{section/introduction.tex}

\input{section/methodology.tex}

\section{Results}

\input{section/benchmark.tex}
% ---------------------------------------------------------------


\section{Discussion} % Conclusions, Further research

\input{section/discussion.tex}

\section*{Acknowledgment}

\input{section/acknowledgment.tex}

\printbibliography[title=References]


\clearpage
\vspace{2cm}

\noindent\textbf{Zoltán Blahovics}\\
Department of Computer Science\\Eötvös Loránd University\\Pázmány Péter Sétány 1/C Budapest, Hungary\\
Budapest\\
Hungary\\
{\tt euxhhx@inf.elte.hu}\\

\noindent\textbf{Bence Nagy}\\
Department of Computer Science\\Eötvös Loránd University\\Pázmány Péter Sétány 1/C Budapest, Hungary\\
Budapest\\
Hungary\\
{\tt hvtdd4@inf.elte.hu}\\

\noindent\textbf{Krisztián Nemes-Kovács}\\
Department of Computer Science\\Eötvös Loránd University\\Pázmány Péter Sétány 1/C Budapest, Hungary\\
Budapest\\
Hungary\\
{\tt omni5q@inf.elte.hu}\\

\noindent\textbf{Balázs Fekete}\\
Department of Computer Science\\Eötvös Loránd University\\Pázmány Péter Sétány 1/C Budapest, Hungary\\
Budapest\\
Hungary\\
{\tt r4jlrz@inf.elte.hu}\\

\end{document}
